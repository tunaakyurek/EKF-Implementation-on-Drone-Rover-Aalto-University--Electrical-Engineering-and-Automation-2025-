%% demo_augmented_rl_agent.m - RL Agent Enhanced with Expert Demonstrations
% PURPOSE
% Extends the standard RL agent to incorporate expert demonstrations for
% faster and more stable learning. Implements demonstration-augmented RL
% as specified in the PDF requirements.

classdef demo_augmented_rl_agent < rl_agent
    properties
        % Expert demonstration integration
        expert_demonstrator
        demo_replay_ratio
        expert_buffer
        
        % Behavior cloning pretraining
        bc_trainer
        bc_pretrained
        
        % Learning statistics
        demo_usage_stats
        bc_loss_weight
    end
    
    methods
        function obj = demo_augmented_rl_agent(obs_dim, action_dim, params, expert_demonstrator)
            % Call parent constructor
            obj@rl_agent(obs_dim, action_dim, params);
            
            % Store expert demonstrator
            obj.expert_demonstrator = expert_demonstrator;
            obj.demo_replay_ratio = params.rl.expert.demo_replay_ratio;
            
            % Initialize expert buffer
            obj.expert_buffer = obj.initialize_expert_buffer();
            
            % Initialize BC trainer
            obj.bc_trainer = behavior_cloning_trainer(params, expert_demonstrator);
            obj.bc_pretrained = false;
            
            % Learning parameters
            obj.bc_loss_weight = 1.0;  % Weight for BC loss in combined learning
            
            % Statistics
            obj.demo_usage_stats = struct();
            obj.demo_usage_stats.expert_samples_used = 0;
            obj.demo_usage_stats.total_samples = 0;
            
            fprintf('Demo-augmented RL agent initialized\n');
            fprintf('  Demo replay ratio: %.2f\n', obj.demo_replay_ratio);
        end
        
        function pretrain_with_behavior_cloning(obj)
            %% Pretrain Agent with Behavior Cloning
            
            if obj.bc_pretrained
                fprintf('Agent already pretrained with behavior cloning\n');
                return;
            end
            
            fprintf('Pretraining agent with behavior cloning...\n');
            
            % Train BC policy
            obj.bc_trainer.train_policy();
            
            % Initialize RL networks with BC weights (simplified approach)
            obj.initialize_networks_from_bc();
            
            obj.bc_pretrained = true;
            fprintf('âœ“ Behavior cloning pretraining complete\n');
        end
        
        function populate_buffer_with_demos(obj)
            %% Populate Replay Buffer with Expert Demonstrations
            
            fprintf('Populating replay buffer with expert demonstrations...\n');
            
            if obj.expert_demonstrator.demo_count == 0
                warning('No expert demonstrations available');
                return;
            end
            
            % Calculate how many demo samples to add
            target_demo_samples = round(obj.buffer_size * obj.demo_replay_ratio);
            added_samples = 0;
            
            % Add demonstrations to replay buffer
            available_demos = min(obj.expert_demonstrator.demo_count, obj.expert_demonstrator.demo_buffer_size);
            
            for demo_idx = 1:available_demos
                demo = obj.expert_demonstrator.demonstrations{demo_idx};
                
                if ~isempty(demo) && demo.success
                    % Add transitions from this demonstration
                    demo_length = size(demo.states, 2);
                    
                    for t = 1:demo_length-1
                        if added_samples >= target_demo_samples
                            break;
                        end
                        
                        % Create experience tuple
                        obs = demo.limited_observations(:, t);
                        action = demo.actions(:, t);
                        reward = demo.rewards(t);
                        next_obs = demo.limited_observations(:, t+1);
                        done = (t == demo_length-1);
                        
                        % Store in replay buffer
                        obj.store_experience(obs, action, reward, next_obs, done);
                        added_samples = added_samples + 1;
                    end
                end
                
                if added_samples >= target_demo_samples
                    break;
                end
            end
            
            fprintf('Added %d expert demonstration samples to replay buffer\n', added_samples);
            obj.demo_usage_stats.expert_samples_used = added_samples;
        end
        
        function train_step(obj)
            %% Enhanced Training Step with Demonstration Augmentation
            
            % Check if we have enough experiences
            if obj.get_buffer_size() < obj.batch_size
                return;
            end
            
            % Sample batch with demo augmentation
            batch = obj.sample_augmented_batch();
            
            % Enhanced training with BC loss
            if obj.bc_pretrained
                critic_loss = obj.train_critic_with_demos(batch);
                actor_loss = obj.train_actor_with_demos(batch);
            else
                % Standard training if not pretrained
                critic_loss = obj.train_critic(batch);
                actor_loss = obj.train_actor(batch);
            end
            
            % Store losses
            obj.critic_losses = [obj.critic_losses, critic_loss];
            obj.actor_losses = [obj.actor_losses, actor_loss];
            
            % Soft update target networks
            obj.soft_update_targets();
            
            obj.step_count = obj.step_count + 1;
            obj.demo_usage_stats.total_samples = obj.demo_usage_stats.total_samples + obj.batch_size;
        end
        
        function batch = sample_augmented_batch(obj)
            %% Sample Batch with Expert Demonstration Augmentation
            
            % Sample regular batch
            batch = obj.sample_batch();
            
            % If we have expert demonstrations, replace some samples
            if obj.expert_demonstrator.demo_count > 0 && obj.demo_replay_ratio > 0
                num_demo_samples = round(obj.batch_size * obj.demo_replay_ratio);
                
                if num_demo_samples > 0
                    % Get expert samples
                    try
                        [~, expert_obs, expert_actions, expert_rewards] = ...
                            obj.expert_demonstrator.get_demonstration_batch(num_demo_samples);
                        
                        % Replace part of the batch with expert samples
                        if size(expert_obs, 2) >= num_demo_samples
                            for i = 1:num_demo_samples
                                batch(i).obs = expert_obs(:, i);
                                batch(i).action = expert_actions(:, i);
                                batch(i).reward = expert_rewards(i);
                                % Keep next_obs and done from regular samples
                            end
                        end
                    catch ME
                        % If demo sampling fails, use regular batch
                        warning('Failed to sample expert demonstrations: %s', ME.message);
                    end
                end
            end
        end
        
        function critic_loss = train_critic_with_demos(obj, batch)
            %% Train Critic with Demonstration Augmentation
            
            % Standard critic training
            critic_loss = obj.train_critic(batch);
            
            % Additional BC regularization could be added here
            % For now, use standard training
        end
        
        function actor_loss = train_actor_with_demos(obj, batch)
            %% Train Actor with Demonstration Augmentation
            
            % Standard actor training
            actor_loss = obj.train_actor(batch);
            
            % Add behavior cloning loss for expert samples
            if obj.bc_loss_weight > 0
                bc_loss = obj.calculate_bc_loss(batch);
                actor_loss = actor_loss + obj.bc_loss_weight * bc_loss;
            end
        end
        
        function bc_loss = calculate_bc_loss(obj, batch)
            %% Calculate Behavior Cloning Loss for Expert Samples
            
            % Simplified BC loss calculation
            % In practice, you'd identify which samples are from expert demos
            % and compute MSE loss between predicted and expert actions
            
            obs_batch = cell2mat({batch.obs}');
            action_batch = cell2mat({batch.action}');
            
            % Predict actions
            predicted_actions = predict(obj.actor_network, obs_batch);
            
            % Calculate MSE loss (simplified)
            bc_loss = mean(sum((predicted_actions - action_batch).^2, 2));
        end
        
        function initialize_networks_from_bc(obj)
            %% Initialize RL Networks from Behavior Cloning Weights
            
            % This is a simplified approach
            % In practice, you'd copy compatible layers from BC network to actor network
            
            if ~isempty(obj.bc_trainer.policy_network)
                fprintf('Initializing actor network from BC policy...\n');
                
                % For now, just report that initialization would happen
                % Full implementation would require careful layer mapping
                fprintf('  BC policy accuracy: %.2f%%\n', obj.bc_trainer.training_accuracy * 100);
                
                % Reduce exploration since we start with a good policy
                obj.noise_scale = obj.noise_scale * 0.5;
                fprintf('  Reduced exploration noise to %.3f\n', obj.noise_scale);
            end
        end
        
        function expert_buffer = initialize_expert_buffer(obj)
            %% Initialize Expert Demonstration Buffer
            
            expert_buffer = struct();
            expert_buffer.observations = [];
            expert_buffer.actions = [];
            expert_buffer.rewards = [];
            expert_buffer.size = 0;
        end
        
        function train_with_demonstrations(obj, environment, num_episodes)
            %% Complete Training Pipeline with Expert Demonstrations
            
            fprintf('=== Demo-Augmented RL Training ===\n');
            
            % 1) Collect expert demonstrations if not available
            if obj.expert_demonstrator.demo_count == 0
                fprintf('Step 1: Collecting expert demonstrations...\n');
                obj.expert_demonstrator.collect_demonstrations(obj.params.rl.expert.demo_collection_episodes);
            else
                fprintf('Step 1: Using existing %d expert demonstrations\n', obj.expert_demonstrator.demo_count);
            end
            
            % 2) Pretrain with behavior cloning
            fprintf('Step 2: Pretraining with behavior cloning...\n');
            obj.pretrain_with_behavior_cloning();
            
            % 3) Populate replay buffer with demonstrations
            fprintf('Step 3: Populating replay buffer with demonstrations...\n');
            obj.populate_buffer_with_demos();
            
            % 4) Standard RL training with demo augmentation
            fprintf('Step 4: RL training with demonstration augmentation...\n');
            obj.train_episodes(environment, num_episodes);
            
            % 5) Evaluation
            fprintf('Step 5: Final evaluation...\n');
            obj.evaluate_final_performance(environment);
            
            fprintf('=== Demo-Augmented Training Complete ===\n');
        end
        
        function train_episodes(obj, environment, num_episodes)
            %% Training Episodes with Demo Augmentation
            
            for episode = 1:num_episodes
                obs = environment.reset();
                episode_reward = 0;
                episode_steps = 0;
                done = false;
                
                while ~done && episode_steps < environment.max_episode_steps
                    % Select action
                    action = obj.select_action(obs, true);
                    
                    % Execute action
                    [next_obs, reward, done, info] = environment.step(action);
                    
                    % Store experience
                    obj.store_experience(obs, action, reward, next_obs, done);
                    
                    % Training step
                    obj.train_step();
                    
                    obs = next_obs;
                    episode_reward = episode_reward + reward;
                    episode_steps = episode_steps + 1;
                end
                
                % Update curriculum
                obj.update_curriculum(episode_reward);
                
                % Logging
                if mod(episode, 50) == 0
                    demo_ratio = obj.demo_usage_stats.expert_samples_used / max(1, obj.demo_usage_stats.total_samples);
                    fprintf('Episode %d: Reward=%.1f, Demo ratio=%.3f, Noise=%.3f\n', ...
                            episode, episode_reward, demo_ratio, obj.noise_scale);
                end
                
                % Decay BC loss weight over time
                if obj.bc_loss_weight > 0.1
                    obj.bc_loss_weight = obj.bc_loss_weight * 0.995;  % Gradual decay
                end
            end
        end
        
        function results = evaluate_final_performance(obj, environment)
            %% Evaluate Final Performance
            
            % Set to evaluation mode
            obj.training_mode = false;
            
            num_eval_episodes = 20;
            results = struct();
            results.episodes = [];
            results.success_count = 0;
            
            fprintf('Final evaluation (%d episodes)...\n', num_eval_episodes);
            
            for episode = 1:num_eval_episodes
                obs = environment.reset();
                episode_reward = 0;
                episode_steps = 0;
                done = false;
                
                while ~done && episode_steps < 1000
                    action = obj.select_action(obs, false);  % No exploration
                    [obs, reward, done, info] = environment.step(action);
                    episode_reward = episode_reward + reward;
                    episode_steps = episode_steps + 1;
                end
                
                episode_result = struct();
                episode_result.success = info.goal_reached;
                episode_result.reward = episode_reward;
                episode_result.steps = episode_steps;
                
                results.episodes = [results.episodes, episode_result];
                if episode_result.success
                    results.success_count = results.success_count + 1;
                end
            end
            
            results.success_rate = results.success_count / num_eval_episodes;
            results.avg_reward = mean([results.episodes.reward]);
            
            fprintf('Final Results:\n');
            fprintf('  Success Rate: %.1f%% (%d/%d)\n', results.success_rate * 100, results.success_count, num_eval_episodes);
            fprintf('  Average Reward: %.2f\n', results.avg_reward);
            fprintf('  Expert Demo Usage: %.1f%% of training samples\n', ...
                    obj.demo_usage_stats.expert_samples_used / max(1, obj.demo_usage_stats.total_samples) * 100);
            
            % Restore training mode
            obj.training_mode = true;
        end
        
        function save_model(obj, filepath)
            %% Save Enhanced Model with Demo Info
            
            % Call parent save
            save_model@rl_agent(obj, filepath);
            
            % Add demo-specific data
            loaded = load(filepath);
            model_data = loaded.model_data;
            model_data.demo_augmented = true;
            model_data.demo_usage_stats = obj.demo_usage_stats;
            model_data.bc_pretrained = obj.bc_pretrained;
            
            save(filepath, 'model_data');
            fprintf('Demo-augmented model saved to: %s\n', filepath);
        end
    end
end
